ex1:
    - Generate franke-data with and without noise
    - Split the data in train-test
    - Use YOUR OWN ols-code to fit the franke-data with a multivariable polynomial in x and y up to 5. order
    - Find the confidence intervals for the beta values using sigma*(X^T @ X)^(-1)
    - Calculate MSE and R^2
    - Do all with and without scaling
    - Discuss if scaling is needed
    
    Extra:
        -> Use own code for scaling
        -> Use own code for tts

ex2:
    - DONE - Generate franke-data (with and without noise)
    - DONE - Split the data in train-test
    - DONE - Use ols-code (we can choose either our own or sklearn)
    - DONE - Replicate the firgure in Hastie et al. that shows how complexity of the model (i.e., the polynomial degree in our case) will increase the MSE for the test data
    - Show (analytically) bias-variance-decomposition (abbreviated BVD)
    - Explain the different terms in the BVD in detail
    - DONE - Calculate the bias-variance-MSE for the franke-data
        -> Do this for different amount of datapoints
        -> Do this for different train/test-size
        -> Everything must be done using bootstrap
        -> Create own code for bootstrap
    - Discuss the bias-variance-MSE-results (bias-variance trade-off (abbreviated BVTO))
    
    Extra:
        -> Test putting  in f instead of y in BVD

ex3:
    - DONE - Write own code for cross-validation (abbreviated CV) for kfold
    - DONE - Repeate ex2 with CV and compare with sklearn's CV and our own bootstrap

ex4:
    - DONE - Write own code for Ridge
    - DONE - Do ex2 and ex3 with Ridge
    - DONE - Compare with ex2 and ex3 (for normal ols)
    - Discuss BVTO as a function of the lambda values
        -> NB! Here we MUST use bootstrap
    
ex5:
    - DONE - Do ex4 with lasso
    - Discuss which model fits best
    - Do BVTO with bootstrap and do MSE analysis with CV

ex6:
    - DONE - Do ex1 to ex5 for real terrain data
    - Use CV to say which model fits best
