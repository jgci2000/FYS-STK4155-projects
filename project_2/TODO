Part a:
    - Write our own Stochastic Gradient Descent (SGD)
    - Use it on OLS and Ridge
    - Fit Franke function with OLS and Ridge using SGD
    - Discuss the results as function of hyperparameter lambda and learningrate gamma

    Extra:
        -> Implement momentum SGD
        -> Implement RMSprop
        -> Implement ADAgrad
        -> Implement ADAM

Part b:
    - Write own Feed Forward Neural Network (FFNN)
        -> Have flexible number of hidden layers
        -> Use sigmoid as activation function for hidden layers
			> Will need to be flexible enough to use any activation function rather than just sigmoid - function pointer might be the best way to pass that in
    - Find out how to initialize the biases
    - Find a fitting activation function for final layer
    - Find good cost function for the FFNN (discuss the choice)
		> See http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function
    - Train the network
    - Test/compare/discuss agains OLS, Ridge and sklearn's FFNN
    - Analyse regularization parameters and the learning rate for optimal MSE/R^2 score

    Extra:
        -> Test our FFNN agains TensorFlow's FFNN

Part c:
    - Try RELU and Leaky RELU as activation for hidden layers instead of Sigmoid
    - Try different ways of initializing weights and biases

Part d:
    - Use our own FFNN to analyse breast cancer data for different activation functions (for both hidden and output layer), # of hidden layers and # of nodes.
    - Discuss the results for different learning rate and regularization parameter

    Extra:
        -> Compare with sklearn, TensorFlow and pytorch
Part e:
    - Define cost function and design matrix (NB! This must be done first (according to problem text)!)
    - Write own logistic regression using SGD
    - Study results as function of learning rate
    - Add l_2 regularization parameter
    - Compare with own FFNN and sklearn's logistic regression

    Extra:
        -> Use normal gradient descent instead of SGD

Part f:
    - Summarize various algorithms and evaluate pros and cons
    - Decide which algo is best for regression and which is best for classification



---

Results to compute

- [ ] use SGD and GD on ridge/ols
- [ ] comparison of NN with sklearn
- [ ] grid search for lambda/eta for Franke
- [ ] different layer/node counts for Franke (layers_comparison.py)
- [âœ“] different activation functions for Franke (activation_fn_comparisons.py)
- [ ] different bias initialization for Franke (biases_init_comparison.py)

- [ ] grid search for lambda/eta for MNIST
- [ ] different layer/node counts for MNIST
- [ ] different activation functions for MNIST
- [ ] grid search for lambda/eta for cancer
- [ ] different layer/node counts for cancer
- [ ] different activation functions for cancer
